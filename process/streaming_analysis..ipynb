{
  "metadata": {
    "name": "spotify_stream_pr",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nspark.version\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsc"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsc.version"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, col, expr\nfrom pyspark.sql.types import StructType"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# show streaming history 0,1 data type\n\n# Sparksession\nspark \u003d SparkSession.builder.getOrCreate()\n\nPATH0\u003d\"/user/mijin/spotify/src/json/StreamingHistory0.json\"\nPATH1\u003d\"/user/mijin/spotify/src/json/StreamingHistory1.json\"\n\ndf0 \u003d spark.read.option(\"multiline\",\"true\").json(PATH0) # !!!!!!!!!!\ndf0.createOrReplaceTempView(\"df_streaming0\")\ndf0.printSchema()\n\ndf1 \u003d spark.read.option(\"multiline\", \"true\").json(PATH1) # !!!!!!!!\ndf1.createOrReplaceTempView(\"df_streaming1\")\ndf1.printSchema()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 1)merge streaming dataframes\nspark.sql(\"SELECT * FROM df_streaming0 UNION SELECT * FROM df_streaming1\").createOrReplaceTempView(\"df_stream\")\nprint(\"done\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- see how many columns in the table\nSELECT COUNT(*) FROM df_stream"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- create UniqueID column (key)\nSELECT *, CONCAT(artistName, \u0027:\u0027, trackName) AS UniqueID \nFROM df_stream\nLIMIT 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 2) Creating table included UniqueID\nspark.sql(\"SELECT *, CONCAT(artistName, \u0027:\u0027, trackName) AS UniqueID FROM df_stream\").createOrReplaceTempView(\"df_stream_unique\")\nprint(\"done\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT * FROM df_stream_unique limit 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- 3) add mnPlayed(minute) column and split \u0027endTime\u0027 column along with endTime/endDate\n\nSELECT artistName,\n       split(endTime, \u0027 \u0027)[0] AS endDate,\n       split(endTime, \u0027 \u0027)[1] AS endTime,\n       msPlayed, \n       ROUND((msPlayed/60000),1) AS mnPlayed, \n       trackName, \n       UniqueID \nFROM df_stream_unique \nLIMIT 20;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 4) create final df_stream table\nspark.sql(\"\"\"SELECT artistName,\n       split(endTime, \u0027 \u0027)[0] AS endDate,\n       split(endTime, \u0027 \u0027)[1] AS endTime,\n       msPlayed,\n       ROUND((msPlayed/60000),1) AS mnPlayed,\n       trackName,\n       UniqueID\n       FROM df_stream_unique\"\"\").createOrReplaceTempView(\"df_stream_fin\")\nprint(\u0027done\u0027)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT *\nFROM df_stream_fin\nLIMIT 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT artistName,\n       trackName,\n       endDate,\n       endTime,\n       mnPlayed,\n       msPlayed,\n       UniqueID\nFROM df_stream_fin LIMIT 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 5) create final df_stream to store as parquet type \ndf_stream \u003d spark.sql(\"\"\"\nSELECT artistName,\n       trackName,\n       endDate,\n       endTime,\n       mnPlayed,\n       msPlayed,\n       UniqueID\nFROM df_stream_fin\n\"\"\")\n\ndf_stream.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 6) store final streaming file as parquet @hdfs\ndf_stream.write.parquet(\u0027/user/mijin/spotify/src/parquet\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.types import StructType, ArrayType, StringType, StructField\n\n# 1) show myLibrary.json data type\n\nPATH2\u003d\"/user/mijin/spotify/src/json/YourLibrary.json\"\n\ndf2 \u003d spark.read.option(\"multiline\",\"true\").json(PATH2) # !!!!!!!!!!\ndf2.createOrReplaceTempView(\"df_library\")\ndf2.printSchema()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# 2) divide with first-level keys \u0027track\u0027\n\nfrom pyspark.sql.functions import col, explode\n\ndf_tracks \u003d df2.select(\n    explode(\"tracks\").alias(\"track\")\n).select(\n     col(\"track.album\").alias(\"album\"),\n     col(\"track.artist\").alias(\"artist\"),\n     col(\"track.track\").alias(\"track\"),\n     col(\"track.uri\").alias(\"uri\")\n)\n\ndf_tracks.createOrReplaceTempView(\"lib_tracks\")\ndf_tracks.printSchema()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT* FROM lib_tracks;"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT *, CONCAT(artist, \u0027:\u0027, track) AS UniqueID \nFROM lib_tracks\nLIMIT 10;"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 3) Creating library table included UniqueID, track_uri\nspark.sql(\"SELECT album, artist, track, CONCAT(artist, \u0027:\u0027, track) AS UniqueID, split(uri, \u0027:\u0027)[2] AS track_uri FROM lib_tracks\").createOrReplaceTempView(\"df_lib_tracks\")\nprint(\"done\")"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT * FROM df_lib_tracks limit 5;"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 4)create final df_track to store as parquet type \ndf_track \u003d spark.sql(\"\"\"\nSELECT album, artist, track, CONCAT(artist, \u0027:\u0027, track) AS UniqueID, split(uri, \u0027:\u0027)[2] AS track_uri FROM lib_tracks\n\"\"\")\n\ndf_track.show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 5) store df_track @hdfs\ndf_track.write.parquet(\u0027/user/mijin/spotify/src/parquet/lib\u0027)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- 1) left join two tables (streaming and lib) to find track_uri\nSELECT artistName,endDate,endTime,msPlayed,mnPlayed,trackName,s.UniqueID,l.album,l.track_uri\nFROM df_stream_fin s\nLEFT JOIN df_lib_tracks l ON s.UniqueID \u003d l.UniqueID "
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# 2)create df_join to store as parquet type\n\ndf_join \u003d spark.sql(\"\"\"\nSELECT artistName,endDate,endTime,msPlayed,mnPlayed,trackName,s.UniqueID,l.album,l.track_uri\nFROM df_stream_fin s\nLEFT JOIN df_lib_tracks l ON s.UniqueID \u003d l.UniqueID \n\"\"\")\n\ndf_join.show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 3) store df_join as parquet type @hdfs\ndf_join.write.parquet(\u0027/user/mijin/spotify/src/parquet/join/\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#1) load genre file derived from spotify api\nPATH_G \u003d \"/user/mijin/spotify/src/genre\"\n\ndf_g \u003d spark.read.option(\"header\", True).csv(PATH_G) \ndf_g.createOrReplaceTempView(\"df_genre\")\ndf_g.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# 2) create genre table\nspark.sql(\"SELECT track_uri, artist_uri, genres FROM df_genre\").createOrReplaceTempView(\"df_genre\")\nprint(\"done\")"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT * FROM df_genre limit 20;"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# prep for visualization 1) load streaming parquet file for visualization\ndfs_parquet \u003d spark.read.parquet(\u0027/user/mijin/spotify/src/parquet/join/*\u0027)\ndfs_parquet.cache\ndfs_parquet.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndfs_parquet.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# prep for visualization 2) make table for visaulization using sql\ndfs_parquet.createOrReplaceTempView(\"my_streaming\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT * FROM my_streaming \nWHERE track_uri is not NULL \nLIMIT 50;"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT COUNT(*) FROM my_streaming\nWHERE mnPlayed \u003e 15"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# prep for visualization 3) make table where listend over 2mins\n\nspark.sql(\"\"\"\nSELECT * FROM my_streaming\nWHERE mnPlayed \u003e 2\" AND mnP\"\").createOrReplaceTempView(\"my_streaming_over2\") ;"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT COUNT(*) FROM my_streaming_over2;\n\n-- original : 13,552\n-- actual :  11,160\n-- differ : 2,392 (random songs popped out not my style?)"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# prep for visualization 4) make table where listend less than 15mins\n\nspark.sql(\"\"\"\nSELECT * FROM my_streaming_over2\nWHERE mnPlayed \u003c\u003d 15\"\"\").createOrReplaceTempView(\"my_streaming_only_songs\") \nprint(\u0027done\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT COUNT(*) FROM my_streaming_only_songs;\n\n-- actual :  11,160\n-- less than 15mins : 11,061\n-- differ : 99 (maybe podcasts?)"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT * FROM my_streaming_only_songs LIMIT 10 ;"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n--by artist\nSELECT artistName,\n       COUNT(artistName) count_artist,\n       ROUND(SUM(mnPlayed),1) mnPlayed_arist,\n       ROUND(SUM(msPlayed/3600000),1) hrPlayed_artist\nFROM my_streaming_only_songs\nGROUP BY artistName\nORDER BY COUNT(artistName) DESC\nLIMIT 20;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n-- by track\nSELECT trackName,\n       artistName,\n       count(trackName) AS track_count,\n       ROUND(SUM(mnPlayed),1) mnPlayed_track,\n       \n       ROUND(SUM(msPlayed/3600000),1) hrPlayed_track\nFROM\n     my_streaming_only_songs\nGROUP BY trackName, artistName\nORDER BY track_count DESC\nLIMIT 20;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n--morning time top list\nSELECT trackName,\n       artistName,\n       COUNT(*) as track_count,\n       ROUND(SUM(mnPlayed),1) mnPlayed_track,\n       ROUND(SUM(msPlayed/3600000),1) hrPlayed_track\n       \nFROM(\n    SELECT trackName,\n           artistName,\n           endTime,\n           mnPlayed,\n           msPlayed\n    FROM my_streaming_only_songs\n    WHERE endTime LIKE \u002705:%\u0027 OR \n          endTime LIKE \u002706:%\u0027 OR \n          endTime LIKE \u002707:%\u0027 OR \n          endTime LIKE \u002708:%\u0027 OR \n          endTime LIKE \u002709:%\u0027 OR \n          endTime LIKE \u002710:%\u0027 OR \n          endTime LIKE \u002711:%\u0027)\n       \n       \n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\n"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Define the schema for the \u0027albums\u0027 and \u0027tracks\u0027 keys\nschema2 \u003d StructType([\n    StructField(\"albums\", ArrayType(StructType([\n        StructField(\"album\", StringType(), nullable\u003dTrue),\n        StructField(\"artist\", StringType(), nullable\u003dTrue),\n        StructField(\"uri\", StringType(), nullable\u003dTrue)\n    ])), nullable\u003dTrue),\n    StructField(\"tracks\", ArrayType(StructType([\n        StructField(\"album\", StringType(), nullable\u003dTrue),\n        StructField(\"artist\", StringType(), nullable\u003dTrue),\n        StructField(\"track\", StringType(), nullable\u003dTrue),\n        StructField(\"uri\", StringType(), nullable\u003dTrue)\n    ])), nullable\u003dTrue)\n])\n\n# Read the JSON file with the specified schema\ndf \u003d spark.read.schema(schema2).json(\"path/to/my_library.json\")\n\n# Create a temporary view or table with the name \u0027my_library\u0027\ndf.createOrReplaceTempView(\"my_library\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "-----------digging from below--------------------"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#삽질\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\n\nspark \u003d SparkSession.builder.getOrCreate()\n\nschema \u003d StructType([\n    StructField(\"endTime\", StringType(), nullable\u003dTrue),\n    StructField(\"artistName\", StringType(), nullable\u003dTrue),\n    StructField(\"trackName\", StringType(), nullable\u003dTrue),\n    StructField(\"msPlayed\", LongType(), nullable\u003dTrue)\n])\n\ndf \u003d spark.read.schema(schema).json(\"file:///home/mijin/space/spotify/json/StreamingHistory0.json\")\ndf.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\n\nschema \u003d StructType([\n     StructField(\"endTime\", LongType(), True) \n    StrucField(\"artistName\", StringType(), True),\n    StructField(\"trackName\", StringType(), True),\n    StructField(\"msPlayed\", LongType(), True)\n    StructFie])\nspark \u003d SparkSession.builder.getOrCreate()\n\ndf \u003d spark.read.json(\"file:///home/mijin/space/spotify/json/StreamingHistory0.json\",\ndf.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport requests\r\n\r\nSPOTIFYHOME \u003d \"file:///home/mijin/space/spotify\"\r\n\r\n\r\n# read my 1+ StreamingHistory files (depending on how)\r\ndf_stream0 \u003d pd.read_json(f\"{SPOTIFYHOME}/json/StreamingHistory0.json\")\r\ndf_stream1 \u003d pd.read_json(f\"{SPOTIFYHOME}/json/StreamingHistory1.json\")\r\n\r\n# merge streaming dataframes\r\ndf_stream \u003d pd.concat([df_stream0,df_stream1])\r\n\r\n# create a \u0027uniqueID\u0027 for each song by combining the fields \u0027artistName\u0027 and \u0027trackName\u0027\r\ndf_stream[\u0027UniqueID\u0027] \u003d df_stream[\u0027artistName\u0027]+\":\"+df_stream[\u0027trackName\u0027]\r\n\r\ndf_stream.head()"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Define the schema for your DataFrame\nschema \u003d StructType([\n    StructField(\"endTime\", LongType(), True),\n    StructField(\"artistName\", StringType(), True),\n    StructField(\"trackName\", StringType(), True),\n    StructField(\"msPlayed\", LongType(), True)\n    \ndf_stream0 \u003d spark.read.schema(schema).json(\"file:///home/mijin/space/spotify/json/StreamingHistory0.json\").cache()\n\ndf_stream0.printSchema()\ndf_stream0.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Define the schema for my JSON data\nschema \u003d StructType([\n    StructField(\"endTime\", TimestampType(), True),\n    StructField(\"artistName\", StringType(), True),\n    StructField(\"trackName\", StringType(), True),\n    StructField(\"msPlayed\", IntegerType(), True)\n])\n\nSPOTIFYHOME \u003d \"file:///home/mijin/space/spotify\"\ndf \u003d spark.read.schema(schema).json(f\"{SPOTIFYHOME}/json/StreamingHistory0.json\")\ndf.write.parquet(f\"{SPOTIFYHOME}/parquet/StreamingHistory0.parquet\")  # Save the DataFrame as a Parquet file\n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nSPOTIFYHOME \u003d \"file:///home/mijin/space/spotify\"\ndf \u003d spark.read.parquet(f\"{SPOTIFYHOME}/parquet/StreamingHistory0.parquet\")\ndf.show(50)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}